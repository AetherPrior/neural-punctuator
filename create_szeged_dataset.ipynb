{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"D:/Data/neural-punctuator/szeged/\"\n",
    "file_path = data_path + \"szeged.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82100,\n",
       " ' A szállásunk egy Balaton melletti kis üdülőfaluban, Zamárdiban volt, a Postának az üdülőházában.\\n')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text), text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\gbenc/.cache\\torch\\hub\\huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-multilingual-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    escape_words = (' (Laughter)', ' (Applause)', ' (Music)', ' (Cheering)', ' (Singing)', ' (Video)')\n",
    "\n",
    "    for ew in escape_words:\n",
    "        text = text.replace(ew, '')\n",
    "\n",
    "    text = text.replace('!', '.')\n",
    "    text = text.replace(':', ',')\n",
    "    text = text.replace('--', ',')\n",
    "    text = text.replace('-', ',')\n",
    "    text = text.replace(' ,', ',')\n",
    "    text = text.replace('♫', '')\n",
    "\n",
    "    text = re.sub(r'--\\s?--', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    text = text.replace(' ,', ',')\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A szállásunk egy Balaton melletti kis üdülőfaluban, Zamárdiban volt, a Postának az üdülőházában.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [clean_text(t) for t in text]\n",
    "text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 119, 136, 117, 102]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\".?,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2target = {-1: 0,\n",
    "              119: 1, # .\n",
    "              136: 2, # ?\n",
    "              117: 3,  # ,\n",
    "              -2: -1, # will be masked\n",
    "             }\n",
    "target2id = {value: key for key, value in id2target.items()}\n",
    "    \n",
    "    \n",
    "def create_target(encoded):\n",
    "    targets = []\n",
    "    text = []\n",
    "\n",
    "    target = -2 # Always mask after [CLS] token\n",
    "\n",
    "    text.append(encoded[0])\n",
    "    for word in encoded[1:]:\n",
    "        if word in id2target.keys():\n",
    "            target = word\n",
    "        else:\n",
    "            if tokenizer._convert_id_to_token(word).startswith('##'):\n",
    "                target = -2\n",
    "            targets.append(target)\n",
    "#             print(target)\n",
    "#             print(tokenizer._convert_id_to_token(word), '\\t', end=\"\")\n",
    "            \n",
    "            target = -1\n",
    "            text.append(word)\n",
    "\n",
    "    targets.append(target)\n",
    "\n",
    "    targets = [id2target[t] for t in targets]\n",
    "\n",
    "    return text, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1119582, 124808, 10817)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_n = 73_000\n",
    "valid_n = 8_500\n",
    "\n",
    "train_text = ' '.join(text[:train_n])\n",
    "valid_text = ' '.join(text[train_n:train_n+valid_n])\n",
    "test_text = ' '.join(text[train_n+valid_n:])\n",
    "\n",
    "len(train_text.split(' ')), len(valid_text.split(' ')), len(test_text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2487559 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (274098 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (25593 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "train_tokens = tokenizer.encode(train_text)\n",
    "valid_tokens = tokenizer.encode(valid_text)\n",
    "test_tokens = tokenizer.encode(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_targets = create_target(train_tokens)\n",
    "valid_tokens, valid_targets = create_target(valid_tokens)\n",
    "test_tokens, test_targets = create_target(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For backward campatibility\n",
    "train_tokens, train_targets = [train_tokens], [train_targets]\n",
    "valid_tokens, valid_targets = [valid_tokens], [valid_targets]\n",
    "test_tokens, test_targets = [test_tokens], [test_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + 'train_data.pkl', 'wb') as f:\n",
    "    pickle.dump((train_tokens, train_targets), f)\n",
    "with open(data_path + 'valid_data.pkl', 'wb') as f:\n",
    "    pickle.dump((valid_tokens, valid_targets), f)\n",
    "with open(data_path + 'test_data.pkl', 'wb') as f:\n",
    "    pickle.dump((test_tokens, test_targets), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
