{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from transformers import BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'COPYRIGHT',\n",
       " 'data.pkl',\n",
       " 'encoded.pkl',\n",
       " 'fix_metadata.py~',\n",
       " 'README',\n",
       " 'stats.py',\n",
       " 'stats.py~',\n",
       " 'targets.pkl',\n",
       " 'ted_talks-10-Sep-2012.json',\n",
       " 'ted_talks-25-Apr-2012.json',\n",
       " 'ted_users-10-Sep-2012.json',\n",
       " 'ted_users-25-Apr-2012.json',\n",
       " 'test_data.pkl',\n",
       " 'text.pkl',\n",
       " 'texts.pkl',\n",
       " 'train_data.pkl',\n",
       " 'valid_data.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_path = os.environ['DATA_PATH']\n",
    "data_path = \"D:/Downloads/ted_dataset.tar/ted_dataset/\"\n",
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = data_path + \"ted_talks-25-Apr-2012.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = [d['transcript'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2477012"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '\\n'.join(transcripts)\n",
    "len(text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "escape_words = (' (Laughter) ', ' (Applause) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ew in escape_words:\n",
    "    text = text.replace(ew, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace('!', '.')\n",
    "text = text.replace(':', ',')\n",
    "text = text.replace('--', ',')\n",
    "text = text.replace('-', ',')\n",
    "# text = text.replace('\\\"', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall I ask for a show of hands or a clapping of people in different generations? I'm interested in how many are three to 12 years old.None, huh? All right. I'm going to talk about dinosaurs. Do you remember dinosaurs when you were that age?Dinosaurs are kind of funny, you know.We're going to kind of go in a different direction right now. I hope you all realize that. So I'll just give you my message up front, Try not to go extinct.That's it.People ask me a lot , in fact, one of the most asked questions I get is, why do children like dinosaurs so much? What's the fascination? And I usually just say, \"Well dinosaurs were big, different and gone.\" They're all gone. Well that's not true, but we'll get to the goose in a minute. So that's sort of the theme, big, different and gone. The title of my talk, Shape,shifting Dinosaurs, The Cause of a Premature Extinction. Now I assume that we remember dinosaurs. And there's lots of different shapes. Lots of different kinds. A long time ago, back in the early 1900s, museums were out looking for dinosaurs. They went out and gathered them up. And this is an interesting story. Every museum wanted a little bigger or better one than anybody else had. So if the museum in Toronto went out and collected a Tyrannosaur, a big one, then the museum in Ottawa wanted a bigger one and a better one. And that happened for all museums. So everyone was out looking for all these bigger and better dinosaurs. And this was in the early 1900s. By about 1970, some scientists were sitting around and they thought, \"What in the world? Look at these dinosaurs. They're all big. Where are all the little ones?\" And they thought about it and they even wrote papers about it, \"Where are the little dinosaurs?\"Well, go to a museum, you'll see, see how many baby dinosaurs there are. People assumed , and this was actually a problem , people assumed that if they had little dinosaurs, if they had juvenile dinosaurs, they'd be easy to identify. You'd have a big dinosaur and a littler dinosaur. But all they had were big dinosaurs. And it comes down to a couple of things. First off, scientists have egos, and scientists like to name dinosaurs. They like to name anything. Everybody likes to have their own animal that they named.And so every time they found something that looked a little different, they named it something different. And what happened, of course, is we ended up with a whole bunch of different dinosaurs. In 1975, a light went on in somebody's head. Dr. Peter Dodson at the University of Pennsylvania actually realized that dinosaurs grew kind of like birds do, which is different than the way reptiles grow. And in fact, he used the cassowary as an example. And it's kind of cool , if you look at the cassowary, or any of the birds that have crests on their heads, they actually grow to about 80 percent adult size before the crest starts to grow. Now think about that. They're basically retaining their juvenile characteristics very late in what we call ontogeny. So allometric cranial ontogeny is relative skull growth. So you can see that if you actually found one that was 80 percent grown and you didn't know that it was going to grow up to a cassowary, you would think they were two different animals. So this was a problem, and Peter Dodson pointed this out using some duck,billed dinosaurs then called Hypacrosaurus. And he showed that if you were to take a baby and an adult and make an average of what it should look like, if it grew in sort of a linear fashion, it would have a crest about half the size of the adult. But the actual sub,adult at 65 percent had no crest at all. So this was interesting. So this is where people went astray again. I mean, if they'd have just taken that, taken Peter Dodson's work, and gone on with that, then we would have a lot less dinosaurs than we have. But scientists have egos; they like to name things. And so they went on naming dinosaurs because they were different. Now we have a way of actually testing to see whether a dinosaur, or any animal, is a young one or an older one. And that's by actually cutting into their bones. But cutting into the bones of a dinosaur is hard to do, as you can imagine, because in museums bones are precious. You go into a museum and they take really good care of them. They put them in foam, little containers. They're very well taken care of. They don't like it if you come in and want to saw them open and look inside.So they don't normally let you do that. But I have a museum and I collect dinosaurs and I can saw mine open. So that's what I do.So if you cut open a little dinosaur, it's very spongy inside like A. And if you cut into an older dinosaur, it's very massive. You can tell it's mature bone. So it's real easy to tell them apart. So what I want to do is show you these. In North America in the Northern Plains of the United States and the Southern Plains of Alberta and Saskatchewan, there's this unit of rock called the Hell Creek Formation that produces the last dinosaurs that lived on Earth. And there are 12 of them that everyone recognizes , I mean the 12 primary dinosaurs that went extinct. And so we will evaluate them. And that's sort of what I've been doing. So my students, my staff, we've been cutting them open. Now as you can imagine, cutting open a leg bone is one thing, but when you go to a museum and say, \"You don't mind if I cut open your dinosaur's skull do you?\" they say, \"Go away.\"So here are 12 dinosaurs. And we want to look at these three first. So these are dinosaurs that are called Pachycephalosaurs. And everybody knows that these three animals are related. And the assumption is is that they're related like cousins or whatever. But no one ever considered that they might be more closely related. In other words, people looked at them and they saw the differences. And you all know that if you are going to determine whether you're related to your brother or your sister, you can't do it by looking at differences. You can only determine relatedness by looking for similarities. So people were looking at these and they were talking about how different they are. Pachycephalosaurus has a big, thick dome on its head, and it's got some little bumps on the back of its head, and it's got a bunch of gnarly things on the end of its nose. And then Stygimoloch, another dinosaur from the same age, lived at the same time, has spikes sticking out the back of its head. It's got a little, tiny dome, and it's got a bunch of gnarly stuff on its nose. And then there's this thing called Dracorex, Hogwart's Eye. Guess where that came from? Dragon. So here's a dinosaur that has spikes sticking out of its head, no dome and gnarly stuff on its nose. Nobody noticed the gnarly stuff sort of looked alike. But they did look at these three and they said, \"These are three different dinosaurs, and Dracorex is probably the most primitive of them. And the other one is more primitive than the other. It's unclear to me how they actually sorted these three of them out. But if you line them up, if you just take those three skulls and just line them up, they line up like this. Dracorex is the littlest one, Stygimoloch is the middle size one, Pachycephalosaurus is the largest one. And one would think, that should give me a clue.But it didn't give them a clue. Because, well we know why. Scientists like to name things. So if we cut open Dracorex , I cut open our Dracorex , and look, it was spongy inside, really spongy inside. I mean, it is a juvenile and it's growing really fast. So it is going to get bigger. If you cut open Stygimoloch, it is doing the same thing. The dome, that little dome, is growing really fast. It's inflating very fast. What's interesting is the spike on the back of the Dracorex was growing very fast as well. The spikes on the back of the Stygimoloch are actually resorbing, which means they're getting smaller as that dome is getting bigger. And if we look at Pachycephalosaurus, Pachycephalosaurus has a solid dome and its little bumps on the back of its head were also resorbing. So just with these three dinosaurs, you can easily , as a scientist , we can easily hypothesize that it is just a growth series of the same animal. Which of course means that Stygimoloch and Dracorex are extinct.Okay. Which of course means we have 10 primary dinosaurs to deal with. So a colleague of mine at Berkley, he and I were looking at Triceratops. And before the year 2000 , now remember, Triceratops was first found in the 1800s , before 2000, no one had ever seen a juvenile Triceratops. There's a Triceratops in every museum in the world, but no one had ever collected a juvenile. And we know why, right? Because everybody wants to have a big one. So everyone had a big one. So we went out and collected a whole bunch of stuff and we found a whole bunch of little ones. They're everywhere. They're all over the place. So we have a whole bunch of them at our museum.And everybody says it's because I have a little museum. When you have a little museum, you have little dinosaurs.If you look at the Triceratops, you can see it's changing, it's shape,shifting. As the juveniles are growing up, their horns actually curve backwards. And then as they grow older, the horns grow forward. And that's pretty cool. If you look along the edge of the frill, they have these little triangular bones that actually grow big as triangles and then they flatten against the frill pretty much like the spikes do on the Pachycephalosaurs. And then, because the juveniles are in my collection, I cut them open and look inside. And the little one is really spongy. And the middle size one is really spongy. But what was interesting was the adult Triceratops was also spongy. And this is a skull that is two meters long. It's a big skull. But there's another dinosaur that is found in this formation that looks like a Triceratops, except it's bigger, and it's called Torosaurus. And Torosaurus, when we cut into it, has mature\n"
     ]
    }
   ],
   "source": [
    "print(text[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\gbenc/.cache\\torch\\hub\\huggingface_pytorch-transformers_master\n",
      "Using cache found in C:\\Users\\gbenc/.cache\\torch\\hub\\huggingface_pytorch-transformers_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlbertModel(\n",
       "  (embeddings): AlbertEmbeddings(\n",
       "    (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 128)\n",
       "    (token_type_embeddings): Embedding(2, 128)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): AlbertTransformer(\n",
       "    (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "    (albert_layer_groups): ModuleList(\n",
       "      (0): AlbertLayerGroup(\n",
       "        (albert_layers): ModuleList(\n",
       "          (0): AlbertLayer(\n",
       "            (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (attention): AlbertAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "            (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (pooler_activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albert_pretrained = torch.hub.load('huggingface/pytorch-transformers', 'model', 'albert-base-v1')\n",
    "albert_tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'albert-base-v1')\n",
    "albert_pretrained.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.3 µs ± 444 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "albert_tokenizer.tokenize(\" , ! ? . ... .. asd. qwe.. tyu, uio , asdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188 µs ± 2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "albert_tokenizer.encode(\" , ! ? . ... .. asd. qwe.. tyu, uio , asdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 13, 9, 60, 15, 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids = (9, 60, 15)\n",
    "albert_tokenizer.encode(\".?,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + 'encoded.pkl', 'rb') as f:\n",
    "    encoded_texts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] shall i ask for a show of hands or a clapping of people in different generations? i'm interested in how many are three to 12 years old.none, huh? all right. i'm going to talk about dinosaurs. do you remember dinosaurs when you were that age?dinosaurs are kind of funny, you know.we're going to kind of go in a different direction right now. i hope you all realize that. so i'll just give you my message\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albert_tokenizer.decode(encoded_texts[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2target = {-1: 0,\n",
    "              9: 1, # .\n",
    "              60: 2, # ?\n",
    "              15: 3  # ,\n",
    "             }\n",
    "target2id = {value: key for key, value in id2target.items()}\n",
    "    \n",
    "    \n",
    "def create_target(encoded):\n",
    "    targets = []\n",
    "    text = []\n",
    "\n",
    "    target = -1\n",
    "\n",
    "    text.append(encoded[0])\n",
    "    for word in encoded[1:]:#encoded_texts[0]:\n",
    "        if word in target_ids:\n",
    "            target = word\n",
    "        else:\n",
    "            targets.append(target)\n",
    "            target = -1\n",
    "            text.append(word)\n",
    "\n",
    "    targets.append(target)\n",
    "\n",
    "    targets = [id2target[t] for t in targets]\n",
    "\n",
    "    return text, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "targets = []\n",
    "for text in encoded_texts:\n",
    "    text, target = create_target(text)\n",
    "    texts.append(text)\n",
    "    targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(919, 207, 23)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = int(.8*len(texts))\n",
    "test_n = int(.98*len(texts))\n",
    "train_texts = texts[:n]\n",
    "train_targets = targets[:n]\n",
    "valid_texts = texts[n:test_n]\n",
    "valid_targets = targets[n:test_n]\n",
    "test_texts = texts[test_n:]\n",
    "test_targets = targets[test_n:]\n",
    "len(train_texts), len(valid_texts), len(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(data_path + 'train_data.pkl', 'wb') as f:\n",
    "#     pickle.dump((train_texts, train_targets), f)\n",
    "# with open(data_path + 'valid_data.pkl', 'wb') as f:\n",
    "#     pickle.dump((valid_texts, valid_targets), f)\n",
    "# with open(data_path + 'test_data.pkl', 'wb') as f:\n",
    "#     pickle.dump((test_texts, test_targets), f)\n",
    "\n",
    "with open(data_path + 'train_data.pkl', 'rb') as f:\n",
    "    train_texts, train_targets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(data_path + \"tokenized.pkl\", 'wb') as f:\n",
    "#     pickle.dump(tokenized_text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter((t for targets in train_targets for t in targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_for_sampler(train_data):\n",
    "    # used because of the imbalanced dataset\n",
    "    train_data['sector_int'], _ = pd.factorize(train_data['sector'])\n",
    "\n",
    "    class_sample_count = np.array(\n",
    "        [len(np.where(train_data['sector_int'] == t)[0]) for t in np.unique(train_data['sector_int'])])\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in train_data['sector_int']])\n",
    "\n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    samples_weig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2300323,), array([0, 0]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = np.array([t for targets in train_targets for t in targets])\n",
    "targets.shape, targets[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86378348, 0.0553644 , 0.00459675, 0.07625538])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dim = 4\n",
    "\n",
    "weights = np.zeros((output_dim, ))\n",
    "for t in range(output_dim):\n",
    "    count = (targets == t).sum()\n",
    "    weights[t] = count\n",
    "\n",
    "weights /= weights.sum()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -1 \"\" 1986981 0.8637834773638311\n",
      "2 60 ? 10574 0.004596745761356122\n",
      "1 9 . 127356 0.055364398825730125\n",
      "3 15 , 175412 0.07625537804908267\n"
     ]
    }
   ],
   "source": [
    "# .?,\n",
    "s = 0\n",
    "weights = []\n",
    "for key, value in c.items():\n",
    "    s += value\n",
    "for key, value in c.items():\n",
    "    print(key, target2id[key], albert_tokenizer.decode(target2id[key]) if key > 0 else \"\\\"\\\"\", value, value/s)\n",
    "    weights.append(s/value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00463303, 0.87060261, 0.07228361, 0.05248074])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "weights = np.array(weights)\n",
    "weights /= weights.sum()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11683584"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in albert_pretrained.parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
